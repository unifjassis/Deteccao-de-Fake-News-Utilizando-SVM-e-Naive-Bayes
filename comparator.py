from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
from sklearn.model_selection import cross_val_score, cross_validate
import numpy as np

def evaluate_and_compare(models, model_names, X_test, y_test, target_names=["Fake", "Real"]):
    results = {}

    for model, name in zip(models, model_names):
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, target_names=target_names, output_dict=True)

        results[name] = {
            "accuracy": acc,
            "precision": report["Fake"]["precision"],
            "recall": report["Fake"]["recall"],
            "f1-score": report["Fake"]["f1-score"]
        }

        # Terminal output
        print(f"\nüìä Avalia√ß√£o para: {name}")
        print(f"Acur√°cia: {acc:.4f}")
        print(f"Precision (Fake): {report['Fake']['precision']:.4f}")
        print(f"Recall (Fake):    {report['Fake']['recall']:.4f}")
        print(f"F1-Score (Fake):  {report['Fake']['f1-score']:.4f}")

    """
        # Gr√°fico comparativo
        labels = list(results.keys())
        accs = [results[m]["accuracy"] for m in labels]
        precisions = [results[m]["precision"] for m in labels]
        recalls = [results[m]["recall"] for m in labels]
        f1s = [results[m]["f1-score"] for m in labels]

        x = range(len(labels))
        width = 0.2

        plt.figure(figsize=(10, 6))
        plt.bar([i - width for i in x], accs, width=width, label="Acur√°cia")
        plt.bar(x, precisions, width=width, label="Precis√£o")
        plt.bar([i + width for i in x], recalls, width=width, label="Revoca√ß√£o")
        plt.bar([i + 2*width for i in x], f1s, width=width, label="F1-score")

        plt.xticks(x, labels)
        plt.ylim(0, 1.05)
        plt.title("Compara√ß√£o de Desempenho dos Modelos")
        plt.legend()
        plt.grid(True, axis="y", linestyle="--", alpha=0.6)
        plt.tight_layout()
        plt.show()
    """

def cross_validate_models(models, model_names, X, y, cv=5):
    """
    Aplica valida√ß√£o cruzada (k-fold) em m√∫ltiplos modelos e exibe m√©tricas m√©dias.
    """
    print(f"\nüìä Valida√ß√£o cruzada ({cv}-fold):\n")
    results = {}

    for model, name in zip(models, model_names):
        print(f"üîπ Modelo: {name}")
        scores = cross_validate(
            model,
            X,
            y,
            cv=cv,
            scoring=["accuracy", "precision", "recall", "f1"],
            return_train_score=False
        )

        results[name] = {
            "accuracy": np.mean(scores["test_accuracy"]),
            "precision": np.mean(scores["test_precision"]),
            "recall": np.mean(scores["test_recall"]),
            "f1": np.mean(scores["test_f1"])
        }

        print(f"  Acur√°cia m√©dia:  {results[name]['accuracy']:.4f}")
        print(f"  Precis√£o m√©dia:  {results[name]['precision']:.4f}")
        print(f"  Revoca√ß√£o m√©dia: {results[name]['recall']:.4f}")
        print(f"  F1-score m√©dia:  {results[name]['f1']:.4f}\n")

    # Gr√°fico comparativo
    labels = list(results.keys())
    accs = [results[m]["accuracy"] for m in labels]
    precisions = [results[m]["precision"] for m in labels]
    recalls = [results[m]["recall"] for m in labels]
    f1s = [results[m]["f1"] for m in labels]

    x = range(len(labels))
    width = 0.2

    plt.figure(figsize=(10, 6))
    plt.bar([i - width for i in x], accs, width=width, label="Acur√°cia")
    plt.bar(x, precisions, width=width, label="Precis√£o")
    plt.bar([i + width for i in x], recalls, width=width, label="Revoca√ß√£o")
    plt.bar([i + 2*width for i in x], f1s, width=width, label="F1-score")

    plt.xticks(x, labels)
    plt.ylim(0, 1.05)
    plt.title("Compara√ß√£o de Desempenho com Valida√ß√£o Cruzada")
    plt.legend()
    plt.grid(True, axis="y", linestyle="--", alpha=0.6)
    plt.tight_layout()
    plt.savefig("val_cross_modelos.png")
    print("üìà Gr√°fico salvo como 'val_cross_modelos.png'")

